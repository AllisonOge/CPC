{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812cc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogech/CPC/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4b383",
   "metadata": {},
   "source": [
    "### **SimCLR model development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dcf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/sthalles/SimCLR.git & https://github.com/lightly-ai/lightly/blob/master/lightly/loss/ntx_ent_loss.py#L17\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, n_chans,\n",
    "                 dim=128,\n",
    "                 mlp_dim=2048,\n",
    "                 temperature=0.1,\n",
    "                 drop_rate=0.1,\n",
    "                 drop_path_rate=0.1):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.backbone = timm.create_model(\n",
    "            \"resnet18\",\n",
    "            in_chans=n_chans,\n",
    "            num_classes=mlp_dim,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "\n",
    "        hidden_dim = self.backbone.fc.weight.shape[1]\n",
    "        del self.backbone.fc\n",
    "        self.backbone.fc = self._build_mlp(3, hidden_dim, mlp_dim, dim, False)\n",
    "\n",
    "    def _build_mlp(self, num_layers, input_dim, mlp_dim, output_dim, last_bn=True):\n",
    "        mlp = []\n",
    "        for l in range(num_layers):\n",
    "            dim1 = input_dim if l == 0 else mlp_dim\n",
    "            dim2 = output_dim if l == num_layers - 1 else mlp_dim\n",
    "\n",
    "            mlp.append(nn.Linear(dim1, dim2, bias=False))\n",
    "\n",
    "            if l < num_layers - 1:\n",
    "                mlp.append(nn.BatchNorm1d(dim2))\n",
    "                mlp.append(nn.ReLU(inplace=True))\n",
    "            elif last_bn:\n",
    "                # follow SimCLR's design: https://github.com/google-research/simclr/blob/master/model_util.py#L157\n",
    "                # for simplicity, we further removed gamma in BN\n",
    "                mlp.append(nn.BatchNorm1d(dim2, affine=False))\n",
    "\n",
    "        return nn.Sequential(*mlp)\n",
    "\n",
    "    def info_nce_loss(self, z_i: torch.Tensor, z_j: torch.Tensor, temperature=0.1):\n",
    "        \"\"\"\n",
    "        :param z_i: Feature vectors from the first view (batch_size, feature_dim)\n",
    "        :type z_i: torch.Tensor\n",
    "        :param z_j: Feature vectors from the second view (batch_size, feature_dim)\n",
    "        :type z_j: torch.Tensor\n",
    "        :param temperature: Temperature parameter for scaling the similarity\n",
    "        :type temperature: float\n",
    "        \"\"\"\n",
    "        # normalize the feature vectors\n",
    "        z_i = F.normalize(z_i, dim=-1)\n",
    "        z_j = F.normalize(z_j, dim=-1)\n",
    "\n",
    "        logits_00 = torch.einsum('ik,jk->ij', z_i, z_i) / temperature\n",
    "        logits_01 = torch.einsum('ik,jk->ij', z_i, z_j) / temperature\n",
    "        logits_10 = torch.einsum('ik,jk->ij', z_j, z_i) / temperature\n",
    "        logits_11 = torch.einsum('ik,jk->ij', z_j, z_j) / temperature\n",
    "\n",
    "        B, _ = z_i.shape\n",
    "\n",
    "        mask = torch.eye(B, dtype=torch.bool).to(z_i.device)\n",
    "\n",
    "        # remove the diagonals of the self logits matrices\n",
    "        logits_00 = logits_00[~mask].view(B, -1)\n",
    "        logits_11 = logits_11[~mask].view(B, -1)\n",
    "\n",
    "        # concatenate the logits\n",
    "        logits_0100 = torch.cat([logits_01, logits_00], dim=-1)\n",
    "        logits_1011 = torch.cat([logits_10, logits_11], dim=-1)\n",
    "\n",
    "        logits = torch.cat([logits_0100, logits_1011], dim=0)\n",
    "        labels = torch.arange(B, dtype=torch.long).repeat(2).to(z_i.device)\n",
    "\n",
    "        return F.cross_entropy(logits, labels, reduction='mean')\n",
    "\n",
    "    def forward(self, x0: torch.Tensor, x1: torch.Tensor):\n",
    "        feats = self.backbone(torch.cat([x0, x1], dim=0))\n",
    "        z0, z1 = feats.chunk(2)\n",
    "\n",
    "        return self.info_nce_loss(z0, z1, self.temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafaedde",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def7dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdba910",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_chans\": 4,\n",
    "    \"dim\": 128,\n",
    "    \"mlp_dim\": 2048,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"drop_path_rate\": 0.0,\n",
    "    \"temperature\": 0.1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"experiment_name\": f\"simclr_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "    \"num_epochs\": 100,\n",
    "    \"knn_k\": 200,\n",
    "    \"knn_t\": 0.1,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dataset_path\": \"./datasets\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"comment\": \"SimCLR model training\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4232e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R22_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Avoid loading the entire dataset into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5_path,\n",
    "                 input_label=\"iq_data\",\n",
    "                 target_label=\"angles\",\n",
    "                 txfms=None):\n",
    "        self.h5_path = h5_path\n",
    "        self.input_label = input_label\n",
    "        self.target_label = target_label\n",
    "        self.txfms = txfms\n",
    "        self._file = None\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self._file is None:\n",
    "            # open in read-only, latest libver, SWMR if you know no writers\n",
    "            self._file = h5py.File(\n",
    "                self.h5_path, 'r', libver='latest', swmr=True)\n",
    "            self._x = self._file[self.input_label]\n",
    "            self._y = self._file[self.target_label]\n",
    "\n",
    "    def __len__(self):\n",
    "        self._ensure_open()\n",
    "        return self._x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        x = self._x[idx]\n",
    "        y = self._y[idx]\n",
    "        if self.txfms:\n",
    "            x = self.txfms(x)\n",
    "        return x, y\n",
    "\n",
    "    def __del__(self):\n",
    "        if self._file is not None:\n",
    "            self._file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df86367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomZeroMasking:\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2207.03046\n",
    "    :param max_rate: Maximum rate of zero masking.\n",
    "    :type max_rate: float (default = 0.1)\n",
    "    :param dim: Dimension to apply zero masking.\n",
    "    :type dim: int (default = -1)\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_rate=.1, dim=-1):\n",
    "        self.max_rate = max_rate\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        mask_size = torch.randint(\n",
    "            0, int(x.size(self.dim)*self.max_rate) + 1, (1,)).item()\n",
    "        mask = torch.ones_like(x).to(x.device)\n",
    "        mask_idx = [slice(None)] * x.ndim\n",
    "        rand_start_idxs = torch.randint(\n",
    "            0, x.size(self.dim), (max(min((x.size(self.dim) - mask_size) // 5, 5), 1),))\n",
    "        mask_idx[self.dim] = torch.flatten(rand_start_idxs.unsqueeze(1)\n",
    "                                           + torch.arange(mask_size).unsqueeze(0))\n",
    "        mask_idx[self.dim] = mask_idx[self.dim].clamp(0, x.size(self.dim) - 1)\n",
    "        mask[mask_idx] = 0\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "class RandomAntennaDropout:\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2312.04519\n",
    "    :param rate: Dropout rate.\n",
    "    :type rate: float (default = 0.1)\n",
    "    :param arrangement: Channel arrangement. 'interleaved' or 'grouped'\n",
    "    :type arrangement: str (default = 'grouped')\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rate=0.1, arrangement='grouped'):\n",
    "        self.rate = rate\n",
    "        self.arrangement = arrangement\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        x = x.clone()  # avoid modifying the input tensor\n",
    "        if x.ndim == 3:\n",
    "            # Handling x of shape [4, 2, 4096]\n",
    "            mask = (torch.rand(*x.shape[:-2], 1, 1) > self.rate).float()\n",
    "            # Apply mask independently to each [2, 4096] pair\n",
    "            x = x * mask.to(x.device)\n",
    "            return x\n",
    "\n",
    "        num_pairs = x.size(-2) // 2\n",
    "\n",
    "        if self.arrangement == 'interleaved':\n",
    "            mask = (torch.rand(*x.shape[:-2], num_pairs, 1) >\n",
    "                    self.rate).repeat_interleave(2, dim=1)\n",
    "            x = x * mask.to(x.device)\n",
    "\n",
    "        elif self.arrangement == 'grouped':\n",
    "            mask = (torch.rand(*x.shape[:-2],\n",
    "                    num_pairs, 1) > self.rate).float()\n",
    "            x[:num_pairs, :] *= mask.to(x.device)\n",
    "            x[num_pairs:, :] *= mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomCircularShift:\n",
    "    \"\"\"\n",
    "    :param max_shift: Maximum shift as a fraction of the sequence length (1.0 means full length).\n",
    "    :type max_shift: float (default is 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_shift=1.0):\n",
    "        self.max_shift = max_shift\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        T = x.size(-1)\n",
    "        m = int(T * self.max_shift)\n",
    "        shift = torch.randint(-m, m + 1, (1,)).item()\n",
    "        return torch.roll(x, shifts=shift, dims=-1)\n",
    "\n",
    "\n",
    "class Jitter:\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2007.15951\n",
    "    :param var: Variance of the Gaussian noise.\n",
    "    :type var: float (default = 1e-5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var=1e-5):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        noise = torch.randn_like(x).to(x.device) * self.var\n",
    "        return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a665c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRTransform:\n",
    "    \"\"\"Take two random transform of one data\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform1, base_transform2):\n",
    "        self.base_transform1 = base_transform1\n",
    "        self.base_transform2 = base_transform2\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x1 = self.base_transform1(x)\n",
    "        x2 = self.base_transform2(x)\n",
    "        return [x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3404fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4, 2, 1024]) torch.Size([1024, 4, 2, 1024]) torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "aug1 = Compose([\n",
    "    Lambda(lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "    RandomZeroMasking(max_rate=0.2),\n",
    "    RandomAntennaDropout(rate=0.1),\n",
    "    RandomCircularShift(max_shift=0.3),\n",
    "    Jitter(var=1e-5),\n",
    "])\n",
    "\n",
    "aug2 = Compose([\n",
    "    Lambda(lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "    RandomZeroMasking(max_rate=0.2),\n",
    "    RandomAntennaDropout(rate=0.1),\n",
    "    RandomCircularShift(max_shift=0.3),\n",
    "    Jitter(var=1e-5),\n",
    "])\n",
    "\n",
    "train_ds = R22_Dataset(\n",
    "    os.path.join(config[\"dataset_path\"], \"train_preprocessed.h5\"),\n",
    "    input_label=\"iq_data\",\n",
    "    target_label=\"angles\",\n",
    "    txfms=SimCLRTransform(aug1, aug2)\n",
    ")\n",
    "\n",
    "memory_ds = R22_Dataset(\n",
    "    os.path.join(config[\"dataset_path\"], \"train_preprocessed.h5\"),\n",
    "    input_label=\"iq_data\",\n",
    "    target_label=\"angles\"\n",
    ")\n",
    "\n",
    "test_ds = R22_Dataset(\n",
    "    os.path.join(config[\"dataset_path\"], \"test_preprocessed.h5\"),\n",
    "    input_label=\"iq_data\",\n",
    "    target_label=\"angles\"\n",
    ")\n",
    "\n",
    "# test with a small dataset\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "idx = torch.randperm(len(train_ds))[:225*1000]\n",
    "train_ds = torch.utils.data.Subset(train_ds, idx)\n",
    "idx = torch.randperm(len(memory_ds))[:225*1000]\n",
    "memory_ds = torch.utils.data.Subset(memory_ds, idx)\n",
    "idx = torch.randperm(len(test_ds))[:225*10]\n",
    "test_ds = torch.utils.data.Subset(test_ds, idx)\n",
    "\n",
    "prefetch_factor = 2\n",
    "num_workers = 8\n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "memory_loader = DataLoader(\n",
    "    memory_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "(xb1, xb2), yb = next(iter(train_loader))\n",
    "print(xb1.shape, xb2.shape, yb.shape)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c351498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 16,692,864 trainable parameters\n",
      "tensor(5.9410, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model = SimCLR(\n",
    "    n_chans=config[\"n_chans\"],\n",
    "    dim=config[\"dim\"],\n",
    "    mlp_dim=config[\"mlp_dim\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "    drop_rate=config[\"drop_rate\"],\n",
    "    drop_path_rate=config[\"drop_path_rate\"]\n",
    ").to(device)\n",
    "\n",
    "print(\n",
    "    f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "loss = model(xb1.to(device), xb2.to(device))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91c9500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \"\"\"\n",
    "    A class to track the best value of a metric.\n",
    "\n",
    "    :param metric: The name of the metric to track. If 'loss' is in the metric name, the goal is to minimize it.\n",
    "    :type metric: str\n",
    "    :param mode: The mode of tracking. Can be 'auto', 'min', or 'max'. Default is 'auto'.\n",
    "    :type mode: str, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric, mode='auto'):\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.mode_dict = {\n",
    "            'auto': np.less if 'loss' in metric else np.greater,\n",
    "            'min': np.less,\n",
    "            'max': np.greater\n",
    "        }\n",
    "        self.operator = self.mode_dict[mode]\n",
    "        self._best = np.inf if self.operator == np.less else -np.inf\n",
    "\n",
    "    @property\n",
    "    def best(self):\n",
    "        return self._best\n",
    "\n",
    "    @best.setter\n",
    "    def best(self, value):\n",
    "        self._best = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "267218ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(feature, feature_bank, feature_labels, num_classes, k=200, t=0.1):\n",
    "    # feature is [b, d], feature_bank is [d, n] and feature_labels is [n]\n",
    "    sim_mat = torch.mm(feature, feature_bank)  # [b, n]\n",
    "    sim_weight, sim_indices = sim_mat.topk(k, dim=-1)  # [b, k]\n",
    "    sim_labels = torch.gather(feature_labels.expand(\n",
    "        feature.size(0), -1), dim=-1, index=sim_indices)\n",
    "    sim_weight = (sim_weight / t).exp()\n",
    "\n",
    "    # count for each class\n",
    "    one_hot = torch.zeros(feature.size(0) * k, num_classes).to(feature.device)\n",
    "    one_hot = one_hot.scatter(\n",
    "        dim=-1, index=sim_labels.view(-1, 1), value=1.0)  # [b*k, num_classes]\n",
    "    pred_scores = torch.sum(one_hot.view(feature.size(0), -1, num_classes) * sim_weight.unsqueeze(dim=-1), dim=1)  # weighted scores [b, num_classes] # noqa\n",
    "\n",
    "    pred_labels = pred_scores.argsort(\n",
    "        dim=-1, descending=True)  # [b, num_classes]\n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def knn_evaluate(model, memory_loader, test_loader, epoch, config, pbar, writer, device):\n",
    "    feature_bank, feature_labels = [], []\n",
    "    encoder = copy.deepcopy(model.backbone)\n",
    "    encoder.fc = nn.Identity()\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "    for x, y in tqdm(memory_loader, desc=\"Extracting features\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        feature = encoder(x)\n",
    "        # normalize the feature\n",
    "        feature = F.normalize(feature, dim=-1)\n",
    "        feature_bank.append(feature)\n",
    "        feature_labels.append(y)\n",
    "    feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()  # [d, n]\n",
    "    feature_labels = torch.cat(feature_labels, dim=0)  # [n]\n",
    "\n",
    "    # loop over the test set\n",
    "    total_num, top1, top5 = 0, 0, 0\n",
    "    for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        feature = encoder(x)\n",
    "        # normalize the feature\n",
    "        feature = F.normalize(feature, dim=-1)\n",
    "        pred_labels = knn_predict(feature, feature_bank, feature_labels,\n",
    "                                  num_classes=config[\"nclasses\"], k=config[\"knn_k\"], t=config[\"knn_t\"])\n",
    "        top1 += (pred_labels[:, 0] == y).sum().item()\n",
    "        top5 += (pred_labels[:, :5] == y.unsqueeze(1)).sum().item()\n",
    "        total_num += y.size(0)\n",
    "    pbar.write(\n",
    "        f\"Epoch [{epoch}/{config['num_epochs']}] Acc@1: {top1 / total_num * 100:.2f}%, Acc@5: {top5 / total_num * 100:.2f}%\"\n",
    "    )\n",
    "    writer.add_scalar(\n",
    "        f\"test/top1\", top1 / total_num * 100, epoch)\n",
    "    writer.add_scalar(\n",
    "        f\"test/top5\", top5 / total_num * 100, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b536db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARS(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    LARS optimizer, no rate scaling or weight decay for parameters <= 1D.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, trust_coefficient=0.001):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay,\n",
    "                        momentum=momentum, trust_coefficient=trust_coefficient)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for g in self.param_groups:\n",
    "            for p in g['params']:\n",
    "                dp = p.grad\n",
    "\n",
    "                if dp is None:\n",
    "                    continue\n",
    "\n",
    "                if p.ndim > 1:  # if not normalization gamma/beta or bias\n",
    "                    dp = dp.add(p, alpha=g['weight_decay'])\n",
    "                    param_norm = torch.norm(p)\n",
    "                    update_norm = torch.norm(dp)\n",
    "                    one = torch.ones_like(param_norm)\n",
    "                    q = torch.where(param_norm > 0.,\n",
    "                                    torch.where(update_norm > 0,\n",
    "                                                (g['trust_coefficient'] * param_norm / update_norm), one),\n",
    "                                    one)\n",
    "                    dp = dp.mul(q)\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'mu' not in param_state:\n",
    "                    param_state['mu'] = torch.zeros_like(p)\n",
    "                mu = param_state['mu']\n",
    "                mu.mul_(g['momentum']).add_(dp)\n",
    "                p.add_(mu, alpha=-g['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af34bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [01:49<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 3.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Acc@1: 6.93%, Acc@5: 44.49%\n",
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [03:36<5:56:33, 216.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [05:24<5:56:33, 216.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 2.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [07:09<5:56:33, 216.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Acc@1: 8.13%, Acc@5: 48.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [07:09<5:56:33, 216.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [07:10<5:51:09, 214.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [08:58<5:51:09, 214.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 1.8953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [10:44<5:51:09, 214.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Acc@1: 6.89%, Acc@5: 47.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [10:44<5:51:09, 214.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [10:44<5:47:19, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [12:33<5:47:19, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 1.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [14:19<5:47:19, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Acc@1: 6.04%, Acc@5: 44.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [14:19<5:47:19, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [14:19<5:43:43, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [16:08<5:43:43, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 1.5180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [17:53<5:43:43, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Acc@1: 5.33%, Acc@5: 45.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [17:54<5:43:43, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [17:54<5:40:06, 214.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [19:43<5:40:06, 214.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 1.4044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [21:28<5:40:06, 214.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Acc@1: 4.98%, Acc@5: 43.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [21:29<5:40:06, 214.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [21:29<5:36:35, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [23:17<5:36:35, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 1.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [25:03<5:36:35, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Acc@1: 5.47%, Acc@5: 44.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [25:03<5:36:35, 214.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [25:04<5:32:53, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [26:52<5:32:53, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 1.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [28:38<5:32:53, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Acc@1: 6.40%, Acc@5: 43.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [28:38<5:32:53, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [28:39<5:29:24, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [30:27<5:29:24, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 1.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [32:13<5:29:24, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Acc@1: 5.82%, Acc@5: 45.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [32:13<5:29:24, 214.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./experiments/simclr_2025-05-14_06-10-25/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [32:13<5:25:45, 214.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint saved to ./experiments/simclr_2025-05-14_06-10-25/last_checkpoint.pt at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [34:02<5:25:45, 214.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 1.1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [35:15<5:56:25, 235.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     77\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate/epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m     optim\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     80\u001b[0m     epoch,\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mknn_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39moperator(avg_loss, tracker\u001b[38;5;241m.\u001b[39mbest):\n",
      "File \u001b[0;32m~/CPC/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mknn_evaluate\u001b[0;34m(model, memory_loader, test_loader, epoch, config, pbar, writer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(memory_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     feature \u001b[38;5;241m=\u001b[39m encoder(x)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# normalize the feature\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# add the label encoder angles to the config\n",
    "with open(\"label_encoder_angles.yaml\", \"r\") as f:\n",
    "    label_encoder_angles = yaml.safe_load(f)\n",
    "config.update(**label_encoder_angles)\n",
    "# dump the config\n",
    "path = os.path.join(\"./experiments\", config[\"experiment_name\"])\n",
    "os.makedirs(path, exist_ok=True)\n",
    "with open(os.path.join(path, \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "optim = LARS(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"] * config[\"batch_size\"] / 256,\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim,\n",
    "    T_max=config[\"num_epochs\"],\n",
    "    eta_min=1e-6,\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(device=device.type, enabled=True)\n",
    "writer = SummaryWriter(\n",
    "    log_dir=os.path.join(\"./experiments/\", config[\"experiment_name\"]),\n",
    ")\n",
    "tracker = Tracker(\"loss/epoch\", mode=\"min\")\n",
    "\n",
    "start_epoch = 0\n",
    "# resume training from a checkpoint if it exists\n",
    "checkpoint_path = config[\"checkpoint_path\"]\n",
    "if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optim.load_state_dict(checkpoint[\"opt_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"sch_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "step = 0\n",
    "with tqdm(range(start_epoch, config[\"num_epochs\"])) as master_bar:\n",
    "    for epoch in master_bar:\n",
    "        model.train()\n",
    "        avg_loss = 0.0\n",
    "        with tqdm(train_loader, leave=False) as pbar:\n",
    "            for (xb1, xb2), _ in pbar:\n",
    "                xb1, xb2 = xb1.to(device), xb2.to(device)\n",
    "                optim.zero_grad()\n",
    "\n",
    "                with torch.amp.autocast(device_type=device.type,\n",
    "                                        enabled=True):\n",
    "                    loss = model(xb1, xb2)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                # # clip gradients\n",
    "                # scaler.unscale_(optim)\n",
    "                # norm = torch.nn.utils.clip_grad_norm_(\n",
    "                #     model.parameters(), 1.0, norm_type=2\n",
    "                # )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    {\"loss/step\": loss.item(),\n",
    "                     #  \"norm\": norm.item()\n",
    "                     })\n",
    "                writer.add_scalar(\"loss/step\", loss.item(), step)\n",
    "                # writer.add_scalar(\"norm/step\", norm.item(), step)\n",
    "                step += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        avg_loss /= len(train_loader)\n",
    "        writer.add_scalar(\"loss/epoch\", avg_loss, epoch)\n",
    "        master_bar.write(f\"Epoch {epoch}: loss = {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        writer.add_scalar(\n",
    "            \"learning_rate/epoch\",\n",
    "            optim.param_groups[0][\"lr\"],\n",
    "            epoch,\n",
    "        )\n",
    "\n",
    "        # evaluate the model\n",
    "        knn_evaluate(\n",
    "            model,\n",
    "            memory_loader,\n",
    "            test_loader,\n",
    "            epoch,\n",
    "            config,\n",
    "            master_bar,\n",
    "            writer,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        writer.flush()\n",
    "        if tracker.operator(avg_loss, tracker.best):\n",
    "            tracker.best = avg_loss\n",
    "            # Save the model checkpoint\n",
    "            checkpoint_path = os.path.join(\n",
    "                \"./experiments\", f\"{config['experiment_name']}/weights.pth\"\n",
    "            )\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            master_bar.write(f\"Model saved to {checkpoint_path}\")\n",
    "        # save the latest checkpoint\n",
    "        checkpoint_path = os.path.join(\n",
    "            \"./experiments\", f\"{config['experiment_name']}/last_checkpoint.pt\")\n",
    "        torch.save({\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"opt_state_dict\": optim.state_dict(),\n",
    "            \"sch_state_dict\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch}, checkpoint_path)\n",
    "        master_bar.write(\n",
    "            f\"Latest checkpoint saved to {checkpoint_path} at epoch {epoch}\"\n",
    "        )\n",
    "\n",
    "writer.close()\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
